# Apriori Algorithm â€“ Flow Based Explanation

## WHY (Apriori ki zarurat kyun?)
Large transaction databases me:
- Items bahut zyada hote hain
- Har possible combination check karna costly hota hai

Isliye need thi ek aise algorithm ki jo
**early stage par hi useless itemsets ko hata de**.

Isi problem ka solution hai **Apriori Algorithm**.

---

## WHAT (Apriori kya hai?)
Apriori ek **Association Rule Mining algorithm** hai jo:
- Frequent itemsets generate karta hai
- Minimum Support threshold ke basis par kaam karta hai

Simple words:
> Apriori batata hai kaunse items saath me frequently aate hain.

---

## CORE IDEA â€“ APRIORI PROPERTY â­
> **â€œAll subsets of a frequent itemset must also be frequent.â€**

Meaning:
- Agar {A, B, C} frequent hai  
- To {A, B}, {A, C}, {B, C} bhi frequent honge

Aur agar:
- Koi subset frequent nahi hai  
â†’ Uska superset kabhi frequent nahi ho sakta âŒ

Isi rule se **pruning possible hoti hai**.

---

## HOW (Apriori Algorithm â€“ STEP BY STEP FLOW)

### 1ï¸âƒ£ Transaction Database
Input hota hai transactions ka set, jaise:
T1: {Bread, Milk}  
T2: {Bread, Diaper, Beer}  
T3: {Milk, Diaper, Beer}

---

### 2ï¸âƒ£ Candidate 1-Itemsets (C1)
- Har single item ko count kiya jata hai
- Support calculate hota hai

Ye sab **possible candidates** hote hain.

---

### 3ï¸âƒ£ Frequent 1-Itemsets (L1)
- Jo items **minimum support pass** kar lete hain
- Baaki items remove ho jaate hain âŒ

---

### 4ï¸âƒ£ Join Step
- L1 ke itemsets ko join karke
- Larger candidate itemsets bante hain

Example:
{Bread} + {Milk} â†’ {Bread, Milk}

---

### 5ï¸âƒ£ Prune Step â­
- Agar kisi candidate ka **koi bhi subset infrequent** ho
- To us candidate ko **remove** kar dete hain

Ye step Apriori ko **efficient** banata hai.

---

### 6ï¸âƒ£ L2, L3, â€¦ Lk Generation
- Join + Prune process repeat hota hai
- Jab tak koi naya frequent itemset na mile

---

### 7ï¸âƒ£ Association Rule Generation
Frequent itemsets se rules bante hain:

Rule format:
A â†’ B

Measures:
- **Support(A â†’ B)** = P(A âˆª B)
- **Confidence(A â†’ B)** = Support(A âˆª B) / Support(A)

---

## FLOW SUMMARY (Memory Friendly)
**Count â†’ Filter â†’ Join â†’ Prune â†’ Repeat**

Short form:
**C â†’ L â†’ Join â†’ Prune â†’ L**

---

## EXAM DRAWING (2-minute diagram)
Transactions  
â†“  
C1 (Candidates)  
â†“  
L1 (Frequent)  
â†“  
Join  
â†“  
Prune  
â†“  
L2, L3, â€¦ Lk

---

## EXAM REALITY
- Numericals light hote hain
- Support / Confidence basic hote hain
- Flow + Property likhne se marks milte hain

---
!![alt text](../13-Diagrams/03b483f7254ca524ab6b2123249a73a70abbc7dbc3f67f034da6d75f9f2640b6.jpeg)

## EXAM TRIGGER LINE â­
> **â€œApriori uses the downward closure property to efficiently generate frequent itemsets by pruning infrequent candidates early.â€**

---

## ONE-LINE MEMORY TRICK
ğŸ‘‰ **Apriori = Early Pruning**